{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import math"
      ],
      "metadata": {
        "id": "e-MmWcRU946L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_line(d=2, seed=None):\n",
        "    \"\"\"\n",
        "    Generates a random line y = w^T x + b in R^d (w, b ~ Normal(0,1)).\n",
        "    Returns:\n",
        "        w (ndarray): shape (d,), random normal\n",
        "        b (float)  : random normal\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    w = np.random.randn(d)\n",
        "    b = np.random.randn()\n",
        "    return w, b\n",
        "\n",
        "def generate_points(w, b, n=20, seed=None):\n",
        "    \"\"\"\n",
        "    Generates n random points x_i in R^d and corresponding y_i = w^T x_i + b.\n",
        "    No additional noise is added here (but can be added if desired).\n",
        "    Returns:\n",
        "        X (ndarray): shape (n, d)\n",
        "        y (ndarray): shape (n,)\n",
        "    \"\"\"\n",
        "    d = len(w)\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    # For example, sample x_i from N(0, I)\n",
        "    X = np.random.randn(n, d)\n",
        "    # Compute y_i = w^T x_i + b\n",
        "    y = X.dot(w) + b\n",
        "    return X, y\n",
        "\n",
        "def ridge_regression_closed_form(X, y, lam=1.0):\n",
        "    \"\"\"\n",
        "    Fits a ridge regressor (L2-regularized least squares) of the form:\n",
        "        theta = argmin_theta (||X*theta - y||^2 + lam * ||theta||^2)\n",
        "    Here we handle the intercept by augmenting X with an all-ones column.\n",
        "    Returns:\n",
        "        theta (ndarray): shape (d+1,) the [w_est, b_est]\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    # Augment X with a column of 1s for the intercept\n",
        "    X_aug = np.hstack([X, np.ones((n,1))])  # shape (n, d+1)\n",
        "\n",
        "    # Solve (X_aug^T X_aug + lam I) theta = X_aug^T y\n",
        "    A = X_aug.T.dot(X_aug) + lam * np.eye(d+1)\n",
        "    b = X_aug.T.dot(y)\n",
        "    theta = np.linalg.inv(A).dot(b)\n",
        "    return theta\n",
        "\n",
        "def compute_mse_full_dataset(theta, X, w_true, b_true):\n",
        "    \"\"\"\n",
        "    Given learned parameters 'theta' (which is [w_est, b_est]) and the\n",
        "    true line w_true, b_true, compute the MSE over the entire dataset X:\n",
        "\n",
        "    MSE = (1/n) * sum_{i} ( (theta^T [x_i;1]) - (w_true^T x_i + b_true) )^2\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "    w_est = theta[:d]\n",
        "    b_est = theta[d]\n",
        "\n",
        "    # True outputs\n",
        "    y_true = X.dot(w_true) + b_true\n",
        "    # Learned model outputs\n",
        "    y_hat = X.dot(w_est) + b_est\n",
        "\n",
        "    mse = np.mean((y_hat - y_true)**2)\n",
        "    return mse\n",
        "\n",
        "def greedy_subset_both_relevance_diversity(X, z, k, lam=1.0):\n",
        "    \"\"\"\n",
        "    Implements the submodular greedy selection rule:\n",
        "      x_i = argmax_{x in X\\S_{i-1}}\n",
        "             (z^T V_{S_{i-1}}^{-1} x)^2 / (1 + x^T V_{S_{i-1}}^{-1} x)\n",
        "             + lam * log(1 + x^T V_{S_{i-1}}^{-1} x)\n",
        "    where V_{S_{i-1}} = lam*I + sum_{x_j in S_{i-1}} x_j x_j^T.\n",
        "\n",
        "    Inputs:\n",
        "        X  (ndarray): shape (n, d)\n",
        "        z  (ndarray): shape (d,) \"test query\" or reference vector\n",
        "        k  (int)    : number of points to choose\n",
        "        lam(float)  : regularization parameter for both V_S and the formula\n",
        "    Returns:\n",
        "        chosen_indices (list): The indices of X chosen by the submodular greedy method\n",
        "    \"\"\"\n",
        "    n, d = X.shape\n",
        "\n",
        "    chosen_indices = []\n",
        "    # Start with V = lam * I\n",
        "    V = lam * np.eye(d)\n",
        "\n",
        "    # Keep track of which indices remain\n",
        "    remaining = set(range(n))\n",
        "\n",
        "    for _ in range(k):\n",
        "        best_val = -np.inf\n",
        "        best_idx = None\n",
        "\n",
        "        # Invert V once per iteration\n",
        "        V_inv = np.linalg.inv(V)\n",
        "\n",
        "        for idx in remaining:\n",
        "            x = X[idx]\n",
        "            # Evaluate the objective\n",
        "            # (z^T V^{-1} x)^2 / (1 + x^T V^{-1} x) + lam*log(1 + x^T V^{-1} x)\n",
        "            numerator = (z @ V_inv @ x)**2\n",
        "            denominator = 1.0 + x @ V_inv @ x\n",
        "            val = (numerator / denominator) + lam * math.log(1.0 + x @ V_inv @ x)\n",
        "            # val = (numerator / denominator)\n",
        "\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_idx = idx\n",
        "\n",
        "        # Add best_idx to chosen set\n",
        "        chosen_indices.append(best_idx)\n",
        "        remaining.remove(best_idx)\n",
        "\n",
        "        # Update V = V + x best_idx x best_idx^T\n",
        "        x_best = X[best_idx].reshape(d,1)\n",
        "        V += x_best @ x_best.T\n",
        "\n",
        "    return chosen_indices\n"
      ],
      "metadata": {
        "id": "y9MvnAiWC0GD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_experiment():\n",
        "    # 1) Generate the true line parameters w, b (dimension d=2 for illustration)\n",
        "    d = 1\n",
        "    w_true, b_true = generate_line(d=d, seed=42)\n",
        "\n",
        "    # 2) Generate 20 random points from that line\n",
        "    n = 30\n",
        "    X, y = generate_points(w_true, b_true, n=n, seed=123)\n",
        "\n",
        "    # 3) We set k=5\n",
        "    k = 3\n",
        "\n",
        "    # 4) Brute force search for the best subset of size k\n",
        "    #    We fit a ridge regressor on each subset, then compute MSE w.r.t. the *true line*.\n",
        "    #    We'll store the minimal MSE and the best subset of indices.\n",
        "    from itertools import combinations\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_subset = None\n",
        "\n",
        "    all_indices = range(n)\n",
        "    for subset_indices in combinations(all_indices, k):\n",
        "        # Extract the subset\n",
        "        X_sub = X[list(subset_indices)]\n",
        "        y_sub = y[list(subset_indices)]\n",
        "\n",
        "        # Fit ridge regression on these k points\n",
        "        theta_sub = ridge_regression_closed_form(X_sub, y_sub, lam=1.0)\n",
        "\n",
        "        # Compute MSE vs the true line w_true, b_true on all data X\n",
        "        mse_val = compute_mse_full_dataset(theta_sub, X, w_true, b_true)\n",
        "\n",
        "        if mse_val < best_mse:\n",
        "            best_mse = mse_val\n",
        "            best_subset = subset_indices\n",
        "\n",
        "    # 5) Now use the submodular greedy approach\n",
        "    #    We need a \"test query\" vector z. Let's just pick a random one from N(0,I).\n",
        "    np.random.seed(999)\n",
        "    z = np.random.randn(d)\n",
        "\n",
        "    greedy_indices = greedy_subset_both_relevance_diversity(X, z, k=k, lam=1.0)\n",
        "\n",
        "    # Fit ridge regression on the greedy subset\n",
        "    X_greedy = X[greedy_indices]\n",
        "    y_greedy = y[greedy_indices]\n",
        "    theta_greedy = ridge_regression_closed_form(X_greedy, y_greedy, lam=1.0)\n",
        "    greedy_mse = compute_mse_full_dataset(theta_greedy, X, w_true, b_true)\n",
        "\n",
        "    # 6) Print results\n",
        "    print(\"=== Experiment: Linear Model Subset Selection ===\\n\")\n",
        "    # Some intermediate prints:\n",
        "    print(f\"1) Reference line (true) in R^{d}:\")\n",
        "    print(f\"   w_true = {w_true}, b_true = {b_true}\\n\")\n",
        "\n",
        "    print(f\"2) The {n} points (X_i, y_i):\")\n",
        "    for i in range(n):\n",
        "        print(f\"   i={i}, X_i={X[i]}, y_i={y[i]:.4f}\")\n",
        "    print()\n",
        "\n",
        "    print(f\"3) Subset size k = {k}. Brute forcing all subsets to find best MSE...\\n\")\n",
        "\n",
        "    print(f\"4) Best subset found by brute force (indices) = {best_subset}\")\n",
        "    print(f\"   Best subset MSE = {best_mse:.6f}\\n\")\n",
        "\n",
        "    print(\"5) Submodular Greedy subset (indices) =\", greedy_indices)\n",
        "    print(f\"   Greedy subset MSE = {greedy_mse:.6f}\")\n",
        "\n",
        "# Actually run the experiment\n",
        "main_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn88LmsnC50O",
        "outputId": "4c04a6ef-2900-452a-9e94-58a03f437b6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Linear Model Subset Selection ===\n",
            "\n",
            "1) Reference line (true) in R^1:\n",
            "   w_true = [0.49671415], b_true = -0.13826430117118466\n",
            "\n",
            "2) The 30 points (X_i, y_i):\n",
            "   i=0, X_i=[-1.0856306], y_i=-0.6775\n",
            "   i=1, X_i=[0.99734545], y_i=0.3571\n",
            "   i=2, X_i=[0.2829785], y_i=0.0023\n",
            "   i=3, X_i=[-1.50629471], y_i=-0.8865\n",
            "   i=4, X_i=[-0.57860025], y_i=-0.4257\n",
            "   i=5, X_i=[1.65143654], y_i=0.6820\n",
            "   i=6, X_i=[-2.42667924], y_i=-1.3436\n",
            "   i=7, X_i=[-0.42891263], y_i=-0.3513\n",
            "   i=8, X_i=[1.26593626], y_i=0.4905\n",
            "   i=9, X_i=[-0.8667404], y_i=-0.5688\n",
            "   i=10, X_i=[-0.67888615], y_i=-0.4755\n",
            "   i=11, X_i=[-0.09470897], y_i=-0.1853\n",
            "   i=12, X_i=[1.49138963], y_i=0.6025\n",
            "   i=13, X_i=[-0.638902], y_i=-0.4556\n",
            "   i=14, X_i=[-0.44398196], y_i=-0.3588\n",
            "   i=15, X_i=[-0.43435128], y_i=-0.3540\n",
            "   i=16, X_i=[2.20593008], y_i=0.9575\n",
            "   i=17, X_i=[2.18678609], y_i=0.9479\n",
            "   i=18, X_i=[1.0040539], y_i=0.3605\n",
            "   i=19, X_i=[0.3861864], y_i=0.0536\n",
            "   i=20, X_i=[0.73736858], y_i=0.2280\n",
            "   i=21, X_i=[1.49073203], y_i=0.6022\n",
            "   i=22, X_i=[-0.93583387], y_i=-0.6031\n",
            "   i=23, X_i=[1.17582904], y_i=0.4458\n",
            "   i=24, X_i=[-1.25388067], y_i=-0.7611\n",
            "   i=25, X_i=[-0.6377515], y_i=-0.4550\n",
            "   i=26, X_i=[0.9071052], y_i=0.3123\n",
            "   i=27, X_i=[-1.4286807], y_i=-0.8479\n",
            "   i=28, X_i=[-0.14006872], y_i=-0.2078\n",
            "   i=29, X_i=[-0.8617549], y_i=-0.5663\n",
            "\n",
            "3) Subset size k = 3. Brute forcing all subsets to find best MSE...\n",
            "\n",
            "4) Best subset found by brute force (indices) = (3, 6, 16)\n",
            "   Best subset MSE = 0.001832\n",
            "\n",
            "5) Submodular Greedy subset (indices) = [6, 16, 17]\n",
            "   Greedy subset MSE = 0.004369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EC1e0eLSDXAV"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}